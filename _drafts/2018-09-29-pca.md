---
layout: post
title:  "PCA for people who have forgotten Linear Algebra"
categories: probability
---
I am currently taking <a href="https://jmurphy.math.tufts.edu/Teaching/Fall2018/" target="_blank">a Math of Data Analysis class</a> with Prof. James Murphy and we are studying everyone's favorite dimension reduction algorithm: Principal Component Analysis. 

Going into the class with my linear algebra and statistics very rusty, it took me a while to decipher the dense math. I would have appreciated a tutorial that would hold my hand and walk me through the math very slowly, so here we are. *Just remember, there is a lot of math, but it is not hard.* It is just a lot of symbols to look at once, so you have to take it slowly and go step by step.

<b>What does PCA do?</b>

You have data that has a lot of dimensions, and maybe the computation is perhaps too heavy for your computer. What can we do? Preferably, we would like to compress the data without losing any important information. That is, *we want to throw away some data without losing important content.* PCA helps us do that.

TODO: insert picture here

In the above example, we have 2D data, but you can see that it forms somewhat a straight line. Most of the variance in the data is on a different line than the two axes, and if you just draw that line you can discard the original two axes and still preserve most of the identity of the data points. You just condensed 2D data to 1D data! <b>This is literally all that PCA is doing: finding the dimensions which capture the most variance in the data, and translating the dataset into those dimensions.</b>

Mathematically, we would like a matrix $U$ that would transform our dataset X into that lower dimension dataset. We want $UX$.

$U$ is going to be an orthogonal matrix. That means that each column in the matrix is perpendicular to each other. Why orthogonal? Because we don't want redundancy in our dimensions in the sense that we don't want the different dimensions to contain the same information. 

(Todo: picture here again)

Do you see how we could add an infinite number of dimensions and still encode the same information? What a waste. Nope, we want to as few dimensions as possible, and the only dimensions that can't contain overlapping information are perpendicular to each other.

Also, let's call each of our columns in $U$ $u_{1}$, $u_{2}$, ..., $u_{n}$. Our $U$ now looks something like this:

(Todo: Insert picture of U matrix with columns u1, u2, ..., un)

Now, that we have an idea of what our matrix $U$ looks like, lets go around finding the direction where the variance of our data is the most. The formula for variance is $\sigma^2 = \frac{1}{n} \sum_{i=1}^{n} (X_{i}-\mu_{i})^2$. So we want to maximize the equation
\begin{equation}
F(u) = \frac{1}{n} \sum_{i=1}^{n} (u^TX_{i}-u^T.\mu_{i})^2
\end{equation}
where $u^T.X_{i}$ is the projection of $X_{i}$ onto $u$. $u$ here is the orthogonal vector where the variance is the most.

To make our math simpler, lets assume that our data is centered at 0, i.e. $\mu$ = 0. This makes our function easier to look at:

\begin{equation}
F(u) = \frac{1}{n} \sum_{i=1}^{n} (u^T.X_{i})^2
\end{equation}
Nice!
I'm going to transform $(u^TX_{i})^2$ into a form that will be useful to us further down the line:

\begin{equation}
\begin{split}
(u^TX_{i})^2 & = (u^TX_{i}).(u^TX_{i}) \newline
		& = (u^TX_{i}).(u^TX_{i})^T \quad\quad (\because u^TX_{i} \text{ is a scalar. For a scalar k, } k^T = k)\newline
		& = u^TX_{i}(X_{i})^T(u^T)^T  \quad\quad(\because (AB)^T = B^TA^T) \newline
		& = u^TX_{i}(X_{i})^Tu \quad\quad\quad\quad(\because (A^T)^T=A)
\end{split}
\end{equation}

Our function thus becomes
\begin{equation}
F(u) = \frac{1}{n} \sum_{i=1}^{n} u^TX_{i}(X_{i})^Tu
\end{equation}

How do we maximize a function?  Calculas. 

More specifically, if we constrain our $u$ to have $\lVert u \rVert = 1$ and $u^T.u =1$, we can use Lagrange Multipliers. <a href="https://medium.com/@andrew.chamberlain/a-simple-explanation-of-why-lagrange-multipliers-works-253e2cdcbf74" target="_blank">This</a> is a good resource if you have forgotten how they work. 

So, let's differentiate with respect to $u$ and set $F(u) = 0$ to get the constrained maximum.

Also, notice:


Let $\gamma = \frac{1}{n} \sum_{i=1}^{n} X_{i}.X_{i}^T)$
Then,
\begin{equation}

\end{equation}
____

Note to self: I dont need the U matrix stuff, only serves to confuse